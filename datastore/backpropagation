I have very much problem with training a neural network on anything I can setup as judgement criteria.
- One obvious reason is that I do not want to build a mediocre network for anything non-mediocre that I am planning to ever do with that end product.
-- Training should not be a thing in my design. So I am looking for potential direct replacement.
--- I am troubled with this replacement task as well because I am trying to replace it with something that would fit that category of things and I don't think such a category be included in the construction at all.
---- So replacment would rather have to be in one catagory which is just a placeholder for something ... not something like that either.
----- A placeholder is also problematic, so get rid of this criteria as well.
------ Now I will be willing to call of that search because I realized that I should be searching for NOT that kind of thing but something that is very far from that kind of organization of thought.
------- One more attempt at rectifying the situation. Then I would introduce a replacement that does not require training from the outside and training then could only be an implicit possible attribute of the network as a whole, where any replacement would also be handled by the network itself.
------ If there could such an abstraction then it solves my problem very nicely and truly, as the network is being wholeheartedly supported for discovering all of its own needs without possibilities of getting crippled by some outer loop or at least for all those crippling tasks that I could imagine would cause internal conflict with the best knowledge that I have right now.
----- Then it is a data structure of standard layered subsystems with coupling functions that aggregate continuous information from all activated nodes of the network.
---- What activates anything in the network and for what activation from activators given from the outside the ground state of the network constitute a function that can be interpreted also from the outside that exists?
--- There is a clear boundary between at least two kinds of substrates, one is the network architecture (which totally depends on intrinsic attributes that does not require knowledge about them from the outside) and two is the rest of the universe that intersects the boundaries anyhow possible.
-- There is an immediate problem at purely the external environment where from all of the instances that would be considered as external, which ones would constitute the one where we recognize the network existing along one line and all things around that line for which the internal is a model of that extrinsic state?
- Training would have addressed this same problem although at cost of eating into the internal from clearing out more and more of whats near that internal substrate whose shape is known, given the entire external that I have proposed to be ever present always.

Thus eating into the internal where the external is relative to whatever we have been able to consume off of the shape space. The over parameterization is simply the relabeling of the same spontaneity caused as local to locally global propetry whereas the globally local is whatever the loss function is able to at-the-spot inform the trainer.

- I do not want this eating into my body practice and with the biased implications projected at every level until locally unbiased knowledge is gained being a complete anti step towards my problem, I shall clearly solve this problem for my own substrate as local intrinsics + knowledge that is present in my brain, to define a closure map between possible surfaces and impossible surfaces.
-- Surface is the catalyst type terminology used in reference to a computational basis by which a set of computation variables are defined and among those some would constitute the computing medium or rather information medium.
--- It implies that the network must also have such type of setup with whatever complexity that scales its operations but nevertheless would allow a medium via which we would communicate.
---- It also implies that for the network to learn any medium construction that would enable necessary communication between the external and internal, it must inevitably have to have the same computation variables via which its medium is a valid channel and for whose computation would enable the network to be accessible by us at all.
----- Additionally I propose that not only the network should learn, we should too if and when it comes to our future projects.
------ This kind of assertions are already very natural and makes a regular basis for any explanation seeking to understand regularity in a shared surface environment.