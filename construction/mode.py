possible = [
    'reading a file from cpu memory as byte valued object',
    'sample part or whole of the set of bytes at file path',
    'copy part or whole of the byte contents to other compatible memory addresses',
    'create a custom dictionary with choosen named objects as keys and arbitrary other objects as values' 
    'assigning a name to each byte elements and storing them as record element in a dictionary',
    'assigning a name to each byte sequence and storing them as record element in a dictionary',
    'assigning a name to each record element in a dictionary and storing the relation into named variable',
    'create a random association with matrix to some named variable content',
    'performing linear algebra operations on matrix and named content where the variable represents vector entity',
    'storing the matrix-vector product of all such named contents as records in a dictionary',
    'reference to readout of the byte elements for legitimate computation performed over the data at that path',
    'some matrix could be constructed whose product with the named vector represented information corresponds to the linear combination of some unknown other legitimate vector',
    'matrix-matrix product between the ideal constructor of the information variables and approximated random matrix correlates would produce a matrix with correlation intact',
]


import layer, trainer, adaptor

# layer: takes in a single input (size) and produces a fixed point layer
# trainer: ..
# adaptor: ..


possibles += [
    "Given a language model like GPT2, it can be caused to produce more text than it was given as prompt",
    "A pretreined GPT2 model could be caused to follow exact input to output for all prompt to completion within the set of training dataset",
    "For a fixed size buffer and buffer containing integers could be mapped to corresponding larger set of integers generated by language model",
    "For any finite set of integer, the model could be made to associate their input labels to the labels corresponding to any other finite set of integers',
    "The model could be given a representation of one target set of text in terms of its own embedding table where the training over these quickly completes and learning happens in copying the syntactic correlations only",
    "For k different models that are trained on k distinct modes of some fixed k known types, where k classes of generative profile could be constructed from small model instances",
    "For a small model group of size k, we could define a function of at most k^N simultaneous classes where at each step k distinct tokens are produced for any possible candidates in k, upto the maximum length N",
    "It would be sufficient to choose between k observables for 1 < k < M generated token per window for k simultaneous window as set of coarse-graining task",
    "I would like to create k layers with N fixed size for each layers maximum output",
    "The stack of k processes will be running in parallel while their states at each moment gets summarized via a master channel while the k seperate processes are k slave channels",
    "
]



# What requires the stack of layers to be active?
# In what state collected through master describe the underlying causal process for which the N step evolution set in?


